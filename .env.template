# -----------------------------------------------------------------------------
# API Keys
# -----------------------------------------------------------------------------

# LLNL LivAI API key (preferred for hosted models: Claude / GPT)
# Setup docs: https://llnl.servicenowservices.com/ess?id=kb_article_view&sysparm_article=KB0026324#a1
LIVAI_API_KEY=your_livai_key_here

# Optional: some libraries expect OPENAI_API_KEY
# If set, it may be treated as an alias for LIVAI_API_KEY
OPENAI_API_KEY=

# -----------------------------------------------------------------------------
# LLM backend selection
# -----------------------------------------------------------------------------

# Select which backend to use:
#   - "ollama" : local LLaMA models via Ollama (default, offline)
#   - "livai"  : LLNL LivAI gateway (Claude / GPT models)
LLM_BACKEND=livai

# -----------------------------------------------------------------------------
# Model selection
# -----------------------------------------------------------------------------

# Model name to use for the selected backend
#
# Ollama examples (local, provenance-safe):
#   - "llama3.1:8b"    (recommended for ~16 GB RAM)
#   - "llama3.1:70b"  (requires ≥32 GB RAM or substantial GPU VRAM)
#
# LivAI examples (exact names depend on LivAI configuration):
#   - "gpt-5"                 # recommended default for agentic workflows
#   - "claude-sonnet-3.7"     # strongest scientific reasoning / final summaries
#   - "gpt-4.1"               # long-context (up to 1M tokens)
#   - "o4-mini"               # fast, low-cost iteration
#   - "o3-mini"               # efficient STEM / coding tasks
#   - "gpt-4o"                # general-purpose multimodal

LLM_MODEL=gpt-5

# -----------------------------------------------------------------------------
# LLM behavior
# -----------------------------------------------------------------------------

# Controls randomness in model outputs
# 0.0–0.2  → deterministic, best for scientific reasoning (recommended)
# 0.3–0.5  → more exploratory
LLM_TEMPERATURE=0.1

# Maximum number of tokens generated per response
# Increase if responses are getting cut off
LLM_MAX_TOKENS=2048

# -----------------------------------------------------------------------------
# Backend-specific configuration
# -----------------------------------------------------------------------------

# Ollama OpenAI-compatible API endpoint
# Only used when LLM_BACKEND=ollama
OLLAMA_BASE_URL=http://localhost:11434/v1

# LivAI OpenAI-compatible API endpoint
# Only used when LLM_BACKEND=livai
LIVAI_BASE_URL=https://livai-api.llnl.gov/

# -----------------------------------------------------------------------------
# LLM selection
# -----------------------------------------------------------------------------

# LLM backend selection
# Supported: "ollama" (local, default), "openai" (API-based)
LLM_BACKEND=ollama

# Model selection
# Ollama examples (local, provenance-safe):
#   - "llama3.1:8b"   (recommended for ~16 GB RAM)
#   - "llama3.1:70b" (requires ≥32 GB RAM or substantial GPU VRAM)
#
# OpenAI examples (if available):
#   - "gpt-4o-mini"
#   - "gpt-4.1"

LLM_MODEL=llama3.1:8b

# -----------------------------------------------------------------------------
# LLM behavior
# -----------------------------------------------------------------------------

# Controls randomness in model outputs
# 0.0–0.2  → deterministic, best for scientific reasoning (recommended)
# 0.3–0.5  → more exploratory
LLM_TEMPERATURE=0.1

# Maximum number of tokens generated per response
# Increase if responses are getting cut off
LLM_MAX_TOKENS=2048

# -----------------------------------------------------------------------------
# Backend-specific configuration
# -----------------------------------------------------------------------------

# Ollama OpenAI-compatible API endpoint
# Only used when LLM_BACKEND=ollama
OLLAMA_BASE_URL=http://localhost:11434/v1

# Optional OpenAI-compatible base URL (e.g., proxies, gateways, Azure-style setups)
# Usually leave empty
OPENAI_BASE_URL=

# -----------------------------------------------------------------------------
# OpenAI authentication (only required if LLM_BACKEND=openai)
# -----------------------------------------------------------------------------

# Set your OpenAI API key here or via your shell environment
# OPENAI_API_KEY=sk-...
